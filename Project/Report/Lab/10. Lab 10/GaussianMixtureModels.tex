%! Author = antonio
%! Date = 7/2/24
The last model we are going to consider is a generative model, the Gaussian Mixture Model (GMM). This model is based on the assumption that the data is generated by a mixture of K Gaussian distributions.
The GMM density consists of a weighted sum of K Gaussians:
\begin{equation}
\mathbf{X}\thicksim  GMM(\mathbf{M} ,\mathcal{S},w)\implies f_x(x)=\sum_{c=1}^{K}\mathcal{N}(x|\mu_g,\Sigma_g)w_g
\end{equation}
where \( M=[\mu_1...\mu_k]\), \(\mathcal{S}=[\Sigma_1...\Sigma_k]\) and \(w=[w_1...w_k]\) are the parameters of the model.\\
Gaussian components can be viewed as clusters to which the samples belong ( hard or soft), and the cluster label is an unobserved latent random variable.
 We can also define in this case the responsability term that represents the posterior probability that a sample belongs to a certain cluster:
\begin{equation}
    \gamma(z_{n,i})=P(G_i=g| X_i=x)=\frac{f_{x_i,G_i}(x_i,g)}{f_{x_i}(x_i)}=\frac{\mathcal{N}(x_i|\mu_g,\Sigma_g)w_g}{\sum_{g'}\mathcal{N}(x_i|\mu_{g'},\Sigma_{g'})w_{g'}}
\end{equation}
Then assign the sample to the cluster label for which the liability is highest and re-estimate the model parameters based on the cluster assignment. 
We can apply the Expectation-Maximization algorithm to estimate the model parameters. The EM algorithm is an iterative algorithm that consists of two steps:
\begin{itemize}
    \item Expectation stage: estimation of the responsability (given the model parameters \((M_t,S_t,w_t)\))
    \item Maximization step: estimation of new model parameters using the above statistics, estimation continues from an initial value of the model parameters until a certain criterion is met. 
\end{itemize}
The EM algorithm then requires an initial estimate for the GMM parameters, so we use the LBG algorithm to incrementally construct a GMM with 2G components from a GMM with G components. The starting point will be \( (1,\mu,\sigma)\), so we use the empirical mean and covariance matrix of the data set. Then it builds a 2-component model starting from one and from each of the new components 2 more components are generated and so on.
GMM can have differents versions as:
\begin{itemize}
    \item \textbf{The diagonal covariance model:} in this setup, the covariance matrix of each component is assumed to be diagonal, which means the variables are considered independent.
    \item \textbf{The full covariance model:} in this case each component has a full covariance matrix, which means that all possible covariances between the variables are considered.
\end{itemize}

tabelle, immagini, conclusioni