%! Author = antonio
%! Date = 7/2/24
The last model we are going to consider is a generative model, the Gaussian Mixture Model (GMM).
This model is based on the assumption that the data is generated by a mixture of K Gaussian distributions.
The GMM density consists of a weighted sum of K Gaussians:

\begin{equation}
    \mathbf{X}\thicksim  GMM(\mathbf{M} ,\mathcal{S},w)\implies f_x(x)=\sum_{c=1}^{K}\mathcal{N}(x|\mu_g,\Sigma_g)w_g
    \label{eq:GMM}
\end{equation}

where \( M=[\mu_1...\mu_k]\), \(\mathcal{S}=[\Sigma_1...\Sigma_k]\) and \(w=[w_1...w_k]\) of \autoref{eq:GMM} are the parameters of the model.\\
Gaussian components can be viewed as clusters to which the samples belong (hard or soft), and the cluster label is an unobserved latent random variable.
We can also define in this case the responsability term, defined as \autoref{eq:responsability}, that represents the
posterior probability that a sample belongs to a certain cluster:

\begin{equation}
    \gamma(z_{n,i})=P(G_i=g| X_i=x)=\frac{f_{x_i,G_i}(x_i,g)}{f_{x_i}(x_i)}=\frac{\mathcal{N}(x_i|\mu_g,\Sigma_g)w_g}{\sum_{g'}\mathcal{N}(x_i|\mu_{g'},\Sigma_{g'})w_{g'}}
    \label{eq:responsability}
\end{equation}

Then assign the sample to the cluster label for which the responsability is highest and re-estimate the model parameters based on the cluster assignment.
We can apply the Expectation-Maximization algorithm to estimate the model parameters.
The EM algorithm is an iterative algorithm that consists of two steps:
\begin{itemize}
    \item Expectation stage: estimation of the responsability (given the model parameters \((M_t,S_t,w_t)\))
    \item Maximization step: estimation of new model parameters using the above statistics, estimation continues from an
    initial value of the model parameters until a certain criterion is met.
\end{itemize}

The EM algorithm then requires an initial estimate for the GMM parameters, so we use the LBG algorithm to incrementally construct a
GMM with 2G components from a GMM with G components.
The starting point will be \( (1,\mu,\sigma)\), so we use the empirical mean and covariance matrix of the data set.
Then it builds a 2-component model starting from one and from each of the new components 2 more components are generated and so on.
GMM can have differents versions as:
\begin{itemize}
    \item \textbf{The full covariance model:} in this case each component has a full covariance matrix, which means that all possible covariances between the variables are considered.
    \item \textbf{The diagonal covariance model:} in this setup, the covariance matrix of each component is assumed to be diagonal, which means the variables are considered independent.
\end{itemize}

As an initial point in our project, we considered having a maximum component number of 32 and using the two methods mentioned above.
So at first we evaluated the model where the number of components for class 0 and class 1 were equal, obtaining the results shown in
\autoref{tab:GMMSameComponents}.

\begin{table}[h!]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm}}
        \toprule
        \multicolumn{5}{c}{\textbf{GMM Model}} \\
        \midrule
        \multirow{2}{*}{\textbf{(nc0, nc1)}} & \multicolumn{2}{c}{\textbf{Full Cov}} & \multicolumn{2}{c}{\textbf{Diag Cov}} \\
        \cmidrule(lr){2-5}
        & \textbf{minDCF} & \textbf{actDCF} & \textbf{minDCF} & \textbf{actDCF} \\
        \midrule
        (1, 1)   & 0.2629          & 0.3051          & 0.2570          & 0.3022          \\
        (2, 2)   & 0.2170          & 0.2337          & 0.2489          & 0.2674          \\
        (4, 4)   & 0.2161          & 0.2395          & 0.1481          & 0.1687          \\
        (8, 8)   & 0.1786          & 0.1928          & 0.1463          & 0.1809          \\
        (16, 16) & 0.1631          & 0.1766          & 0.1622          & 0.1769          \\
        (32, 32) & 0.2337          & 0.2499          & 0.1766          & 0.1989          \\
        \bottomrule
    \end{tabular}
    \captionsetup{justification=justified,singlelinecheck=false,format=hang}
    \caption{Show minDCF and actDCF for same number of components in GMM}
    \label{tab:GMMSameComponents}
\end{table}

Subsequently, all possible combinations within the maximum value indicated above were tested.
\autoref{tab:GMMBestComponents} shows what may be the most significant values.
I chose these combinations because (1,16) and (2,16) are the two best combinations for the Full Covariance method,
whereas (8,32) and (8,16) are the best for Diagonal Covariance method.

\begin{table}[h!]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm}}
        \toprule
        \multicolumn{5}{c}{\textbf{GMM Model}} \\
        \midrule
        \multirow{2}{*}{\textbf{(nc0, nc1)}} & \multicolumn{2}{c}{\textbf{Full Cov}} & \multicolumn{2}{c}{\textbf{Diag Cov}} \\
        \cmidrule(lr){2-5}
        & \textbf{minDCF} & \textbf{actDCF} & \textbf{minDCF} & \textbf{actDCF} \\
        \midrule
        (1, 16) & 0.1495          & 0.2055          & 0.1433          & 0.1807          \\
        (2, 16) & 0.1701          & 0.1980          & 0.1536          & 0.1731          \\
        (8, 32) & 0.1745          & 0.1903          & 0.1312          & 0.1517          \\
        (8, 16) & 0.1526          & 0.1725          & 0.1324          & 0.1487          \\

        \bottomrule
    \end{tabular}
    \captionsetup{justification=justified,singlelinecheck=false,format=hang}
    \caption{Show minDCF and actDCF for same number of components in GMM}
    \label{tab:GMMBestComponents}
\end{table}