%! Author = antonio
%! Date = 7/2/24

\subsubsection{Linear Support Vector Machines}
Support Vector Machines are linear classifiers that look for maximum margin separation hyperplanes.
The primal formulation of the soft-margin SVM problem consists in minimizing the function:

\begin{equation}
    \mathbf{J}(w,b)= \frac{1}{2}||w||^2 + C\sum_{i=1}^{N}max(0,1-z_i(w^Tx_i+b))
    \label{eq:SVMprimalFormulation}
\end{equation}

where in \autoref{eq:SVMprimalFormulation} N is the number of training samples, C is the regularization parameter,
and \(z_i\) is the margin of the i-th sample.\\
The dual formulation of the problem is:

\begin{equation}
    \mathbf{J}(\alpha)= - \frac{1}{2} \alpha^T \;\textbf{H} \;\alpha + \alpha^T \textbf{1} \;\;\;\;\;\;1\leq \alpha_i\leq C,\;\; \forall i \in \{1,...,N\},\;\; \sum_{i=1}^{n}\alpha_i z_i=0
    \label{eq:SVMdualFormulation}
\end{equation}

where H in  \autoref{eq:SVMdualFormulation} is \(H_{ij}=z_iz_jx_i^Tx_j\) and the dual solution is the maximizer of \(J^D(\alpha)\).\\
Primal and dual solutions are related through:

\begin{equation}
    w^*=\sum_{i=1}^{N}\alpha_i^* z_i x_i
\end{equation}

In addition it's possible to rewrite dual problem as minimization of:
\begin{equation}
    \hat{\mathbf{L}}(\alpha)=-\mathbf{J}(\alpha)= \frac{1}{2} \alpha^T \;\textbf{H} \;\alpha - \alpha^T \textbf{1}
\end{equation}

and it can be minimized by L-BFGS-B algorithm.
After that we have calculated the optimal \(\alpha\) we can compute \(w^*\).

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.30\linewidth}
        \includegraphics[width=\linewidth]{Lab/09. Lab 09/Images/01. L - minDCF}
        \label{fig:LminDCF}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\linewidth}
        \includegraphics[width=\linewidth]{Lab/09. Lab 09/Images/02. L - actDCF}
        \label{fig:LactDCF}
    \end{subfigure}
    \begin{subfigure}[b]{0.30\linewidth}
        \includegraphics[width=\linewidth]{Lab/09. Lab 09/Images/03. L - min And actDCF}
        \label{fig:LminAndActDCF}
    \end{subfigure}
    \caption{Shows minDCF and actDCF for Linear SVM}
    \label{fig:LSVM}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2.5cm} >{\centering\arraybackslash}p{2.5cm} >{\centering\arraybackslash}p{2.5cm}}
        \toprule
        \multicolumn{3}{c}{\textbf{Linear SVM \(K=1.0\)}} \\
        \midrule
        \texbf{C}   & \textbf{minDCF} & \textbf{actDCF} \\
        \midrule
        \(10^{-5}\) & 1.0000          & 1.0000          \\
        \(10^{-4}\) & 0.3640          & 1.0000          \\
        \(10^{-3}\) & 0.3620          & 0.9593          \\
        \(10^{-2}\) & 0.3620          & 0.6718          \\
        \(10^{-1}\) & 0.3582          & 0.5162          \\
        \(10^{0}\)  & 0.3582          & 0.4894          \\
        \bottomrule
    \end{tabular}
    \captionsetup{justification=justified,singlelinecheck=false,format=hang}
    \caption{Show minDCF and actDCF for Linear SVM}
    \label{tab:SVMLin}
\end{table}

\newpage

\subsubsection{Kernel Support Vector Machines}
It's possible in Support Vector Machines to use kernels to allow nonlinear classification.
In this case there is no explicit expansion of the feature space;
we only can calculate the scalar product between the expanded features:\( k(x_1,x_2)=\phi(x_1)_T\phi(x_2)\) where k is the kernel function.
To do this we need to go and replace H as we saw in the previous section with \(\hat{H}= z_iz_jk(x_1,x_2)\).
During our project we see two different types of kernels:
\begin{itemize}
    \item \textbf{Polynomial kernel of degree d}: \(k(x_1,x_2)=(x_1^Tx_2+c)^d\)
    \item \textbf{Radial Basis Function kernel(RBF)}: \(k(x_1,x_2)=e^{-\gamma||x1-x_2||^2}\)
\end{itemize}
We can now apply the polynomial kernel to the SVM with \(d=2,\;\; c=1, \;\;\xi=0\) and see how minDCF and actDCF vary as C changes.


%tabelle e conclusioni